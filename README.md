ğŸš€ Serverless Web Scraper with GitHub Actions
ğŸ“Œ Overview

This project demonstrates a serverless, automated web scraping system built with Python and GitHub Actions.
It is designed to scrape large-scale datasets (3,000+ records per run) from internal-style websites and execute daily without any server infrastructure.

The project reflects a real-world scraping workflow commonly requested by clients on platforms like Upwork.

âœ¨ Key Features

âœ… Daily automated scraping using GitHub Actions (cron jobs)

âœ… Serverless architecture (no VPS, no cloud server)

âœ… Pagination support for large datasets (3,000+ records)

âœ… Modular Python codebase (auth, parser, runner)

âœ… CSV data export

âœ… Production-ready project structure

âœ… Authentication-ready (simulated internal website access)

ğŸ§° Tech Stack

Python 3.10

Requests

BeautifulSoup4

GitHub Actions

Cron scheduling

ğŸ“ Project Structure
.
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ scrape.yml        # GitHub Actions workflow
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ auth.py           # Authentication/session handling
â”‚   â”œâ”€â”€ parser.py         # Pagination scraping logic
â”‚   â””â”€â”€ scraper.py        # Main entry point
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ output.csv        # Scraped data output
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ How It Works

GitHub Actions triggers the workflow daily (or manually).

A Python environment is set up automatically.

The scraper:

Initializes a session (simulating internal website access)

Iterates through paginated pages

Collects structured data

Results are saved as a CSV file.

Updated data is committed back to the repository.

ğŸ“Š Data Scale & Pagination

The scraper supports hundreds of pages via pagination.

Logic is designed to handle 3,000+ records per execution.

Demo website limits the available pages, but the pagination logic is fully scalable and production-ready.

â–¶ï¸ Run Locally
pip install -r requirements.txt
python -m scraper.scraper

ğŸ•’ GitHub Actions Automation

The workflow runs automatically using cron scheduling:

schedule:
  - cron: "0 6 * * *"


This ensures hands-free daily scraping without maintaining any server.

ğŸ§‘â€ğŸ’¼ Client / Portfolio Use Case

This project is ideal for:

Internal website scraping

Scheduled data collection

Serverless automation

Upwork & freelance portfolio demonstration

Example client description:

Built a serverless Python scraping system using GitHub Actions to automatically collect large datasets on a daily schedule without any server infrastructure.

âš ï¸ Disclaimer

This repository uses a public demo website for demonstration purposes only.
The architecture and logic are intended to represent internal or authenticated website scraping workflows.

ğŸ“¬ Contact

If youâ€™re looking for:

Automated web scraping

GitHub Actions automation

Serverless data pipelines

Feel free to reach out.
